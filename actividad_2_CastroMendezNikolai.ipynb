{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9009124b4c18f0eda044abc5e2ff141f",
     "grade": false,
     "grade_id": "cell-20cd3bb8f5a12f07",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Actividad 2: Structured Streaming y Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuerda borrar siempre las líneas que dicen `raise NotImplementedError`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1fed64c5c94f8b4c27326b8b48ec13f",
     "grade": false,
     "grade_id": "cell-2fbae7fd82212064",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Punto de partida (final de la actividad 1): función `retrasoMedio` \n",
    "***Para los vuelos que llegan con retraso positivo, calcular para cada aeropuerto de llegada el retraso medio.***\n",
    "\n",
    "Recordatorio: *El código que calcule esto debería ir encapsulado en una función de Python que reciba como argumento un DataFrame y devuelva como resultado el DataFrame con el cálculo del retraso medio por aeropuerto, ordenado de mayor a menor retraso medio. La columna creada con el retraso medio debe llamarse `retraso_medio`.*\n",
    "\n",
    "**Copia en la siguiente celda el código de tu función retrasoMedio que has completado en la actividad 1**. El DataFrame devuelto por la función debería tener solamente dos columnas: `dest` y `retraso_medio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrasoMedio(df):\n",
    "    c_restra_medio = df.filter(F.col(\"arr_delay\") > 0)\\\n",
    "                                .groupBy(\"dest\")\\\n",
    "                                .agg(F.mean(\"arr_delay\").alias(\"retraso_medio\"))\\\n",
    "                                .sort(\"retraso_medio\", ascending = False)\n",
    "    return c_restra_medio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1\n",
    "\n",
    "Utilizaremos Kafka para actualizar en tiempo real el resultado calculado en el apartado anterior. \n",
    "\n",
    "Para simplificar, asumimos que los mensajes leídos de Kafka tiene solamente dos campos que son los únicos necesarios para llevar a cabo la operación anterior: dest y arr_delay. La idea será crear un Streaming DataFrame para leer de Kafka, y después invocar a nuestra función retrasoMedio pasándolo como argumento. Vamos a leer del topic `retrasos` por lo que debes indicar esta opción a continuación.\n",
    "\n",
    "Se pide crear, en la variable `retrasosStreamingDF`, un Streaming DataFrame leyendo de Apache Kafka, configurando las siguientes opciones:\n",
    "  * Usar la variable `readStream` (en lugar de `read` como solemos hacer) interna de la SparkSession `spark`\n",
    "  * Indicar que el formato es `\"kafka\"` con `.format(\"kafka\")`\n",
    "  * Indicar cuáles son los brokers de Kafka de los que vamos a leer y el puerto al que queremos conectarnos para leer (9092 es el que usa Kafka por defecto), con `.option(\"kafka.bootstrap.servers\", \"<nombre_cluster>-w-0:9092,<nombre_cluster>-w-1:9092\")`. De esa manera podremos leer el mensaje si el productor de Kafka lo envía a cualquiera de los dos brokers existentes, que son los nodos del cluster identificados como `<nombre_cluster>-w-0` y `<nombre_cluster>-w-1`\n",
    "  * Indicar que queremos subscribirnos al topic `\"retrasos\"` con `.option(\"subscribe\", \"retrasos\")`.\n",
    "  * Finalmente ponemos `load()` para realizar la lectura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca3d9ff34a83745e199a3c4aed9acf07",
     "grade": false,
     "grade_id": "read-stream",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Reemplaza por el código correcto siguiendo las indicaciones anteriores\n",
    "retrasosStreamingDF = spark.readStream\\\n",
    "                    .format(\"kafka\")\\\n",
    "                    .option(\"kafka.bootstrap.servers\", \"datanikocluster-w-1:9092\")\\\n",
    "                    .option(\"subscribe\", \"retrasos\")\\\n",
    "                    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb5c34086b3c3fb29a4c46865396bcb1",
     "grade": true,
     "grade_id": "read-stream-tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Mostramos el esquema de este DataFrame\n",
    "types = retrasosStreamingDF.dtypes\n",
    "assert(retrasosStreamingDF.isStreaming)\n",
    "assert((types[0][0] == \"key\")       & (types[0][1] == \"binary\"))\n",
    "assert((types[1][0] == \"value\")     & (types[1][1] == \"binary\"))\n",
    "assert((types[2][0] == \"topic\")     & (types[2][1] == \"string\"))\n",
    "assert((types[3][0] == \"partition\") & (types[3][1] == \"int\"))\n",
    "assert((types[4][0] == \"offset\")    & (types[4][1] == \"bigint\"))\n",
    "assert((types[5][0] == \"timestamp\") & (types[5][1] == \"timestamp\"))\n",
    "assert((types[6][0] == \"timestampType\") & (types[6][1] == \"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrasosStreamingDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "699b1979df4eb0241b1b4ba165d6b311",
     "grade": false,
     "grade_id": "cell-580bf3caf39b314e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Muestra por pantalla el esquema del DataFrame resultante de la lectura con `printSchema()`. Verás que todas estas columnas son creadas automáticamente por Spark cuando leemos de Kafka. De ellas, la que nos interesa es `value` que contiene propiamente el mensaje de Kafka, en formato datos binarios. \n",
    "\n",
    "### Ejercicio 2\n",
    "\n",
    "Tendremos que estructurar estos datos para poder extraer los campos. Para ello sigue los siguientes pasos, ayudándote de la plantilla que hay en la celda siguiente (descoméntala y complétala):\n",
    "\n",
    "* **Selecciona** la columna `value` y conviértela (`.cast`) a `StringType()` utilizando `withColumn` para reemplazar la columna existente `\"value\"` por el objeto Column resultante de la conversión. De esta forma tendremos una columna que contendrá en cada **fila** un **fichero JSON completo**, tal como se muestra en cada una de las plantillas anteriores. \n",
    "* Para extraer los dos campos de cada uno de los JSON y convertirlos en una columna llamada `parejas`, de tipo `struct` (una estructura formada por dos campos de tipo String e Integer respectivamente), utilizamos la función `from_json` de Spark, que se aplica a cada elemento (cada fila) de la columna \"value\" y parsea el String según un esquema que le indiquemos, devolviendo una columna de tipo `struct`.\n",
    "* La columna `parejas` es de tipo `struct` por lo que puedes acceder a cada uno de sus dos campos (`dest` y `arr_delay`) con el operador `.` (punto). Utilizando `withColumn` dos veces, crea dos columnas llamadas `dest` y `arr_delay` como el resultado de acceder a `parejas.dest` y `parejas.arr_delay` respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91b265facf6e3bded458cc9b6a754b1a",
     "grade": false,
     "grade_id": "estructura-json",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "esquema = StructType([\\\n",
    "  StructField(\"dest\", StringType()),\\\n",
    "  StructField(\"arr_delay\", DoubleType())\\\n",
    "])\n",
    "\n",
    "\n",
    "parsedDF = retrasosStreamingDF\\\n",
    "         .select(\"value\")\\\n",
    "         .withColumn(\"value\", F.col(\"value\").cast(StringType()))\\\n",
    "         .withColumn(\"parejas\", F.from_json(F.col(\"value\"), esquema))\\\n",
    "         .withColumn(\"dest\", F.col(\"parejas.dest\"))\\\n",
    "         .withColumn(\"arr_delay\", F.col(\"parejas.arr_delay\"))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6fc63420d404c6ca1fedec52d4ee3b7a",
     "grade": true,
     "grade_id": "estructura-json-tests",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tipos = parsedDF.dtypes\n",
    "assert((\"value\", \"string\") in tipos)\n",
    "assert(('parejas', 'struct<dest:string,arr_delay:double>') in tipos)\n",
    "assert(('dest', 'string') in tipos)\n",
    "assert(('arr_delay', 'double') in tipos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro DataFrame ya contiene una columna `dest` con el nombre del aeropuerto destino y una columna de números reales `arr_delay` con el retraso. Ya podemos efectuar el mismo tipo de agregación que estamos haciendo en nuestra función `retrasoMedio`. Por tanto, invocamos a `retrasoMedio` pasando `parsedDF` como argumento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalúa el siguiente código pero no lo modifiques\n",
    "# Indicamos que este DataFrame se guarde en memoria cuando se va actualizando,\n",
    "# y arrancamos la ejecución en Streaming con la acción start()\n",
    "\n",
    "retrasoMedioStreamingDF = retrasoMedio(parsedDF)\n",
    "\n",
    "consoleOutput = retrasoMedioStreamingDF\\\n",
    "                    .writeStream\\\n",
    "                    .queryName(\"retrasosAgg\")\\\n",
    "                    .outputMode(\"complete\")\\\n",
    "                    .format(\"memory\")\\\n",
    "                    .start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "018ce42cfbc94eef069b4a60e5e89090",
     "grade": false,
     "grade_id": "cell-b073802f3eec8bb3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Una vez evaluada la celda anterior, abre el productor de Kafka console entrando por SSH a cualquiera de las máquinas (revisa el enunciado de la práctica para recordarlo), y copia y pega (literalmente) los siguientes 4 mensajes en formato JSON. Como ves,  tienen un campo `dest` y un campo `arr_delay`, simulando la información que estaríamos recibiendo en tiempo real de los distintos aeropuertos a medida que los vuelos van aterrizando. \n",
    "\n",
    "Cada vez que pegues un mensaje, ejecuta la consulta `select * from retrasosAgg` a través del método `spark.sql(...)` y muestra el DataFrame `agregadosDF` devuelto por dicho método. Eso hará una consulta contra la vista temporal (volátil) `retrasosAgg` que se ha creado en el metastore de Hive gracias al `writeStream` del apartado anterior. Ejecuta la celda de `show` tantas veces como sea necesario hasta ver un resultado distinto al que has visto en la ejecución anterior, para asegurarte de que Spark ya ha leído e incorporado el nuevo dato en su cálculo de la agregación y por tanto ha actualizado el resultado.\n",
    "\n",
    "Recuerda que el método `.sql(...)` es una transformación, y por tanto, se re-ejecuta la consulta cada vez que invocas a la acción `show()` sobre el resultado, ya que **no vamos a cachear nada**, precisamente para forzar la reevaluación de la consulta y poder ver así el contenido actualizado de dicha tabla (en memoria) de Hive cada vez que hacemos `show()`.\n",
    "\n",
    "Se pide: \n",
    "* Cada vez que envíes un mensaje y te hayas asegurado de que Spark ha incorporado el dato a su cálculo, apunta el resultado de la agregación (valor de la columna `retraso_medio`) para MAD y GRX en las variables habilitadas para ello\n",
    "* No te preocupes por evaluar muchas veces una misma celda, ya que el cálculo sólo se actualizará una vez. Las siguientes veces que la evalúes seguirá mostrando el mismo resultado mientras no envíes otro nuevo mensaje en Kafka.\n",
    "\n",
    "Los 4 mensajes que hay que introducir sucesivamente en Kafka son:\n",
    "\n",
    "`\n",
    "{\"dest\": \"GRX\", \"arr_delay\": 2.6}\n",
    "{\"dest\": \"MAD\", \"arr_delay\": 5.4}\n",
    "{\"dest\": \"GRX\", \"arr_delay\": 1.5}\n",
    "{\"dest\": \"MAD\", \"arr_delay\": 20.0}\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aca37e32dce02202f512568edca47c19",
     "grade": false,
     "grade_id": "sql-streaming",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "agregadosDF = spark.sql(\"select * from retrasosAgg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c17f79058004e6c56c6cb64795ae6f2",
     "grade": true,
     "grade_id": "sql-streaming-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "columnas = agregadosDF.columns\n",
    "assert(len(columnas) == 2)\n",
    "assert(\"dest\" in columnas)\n",
    "assert(\"retraso_medio\" in columnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "709560e7fbed0009d74b7fec8c90a3c1",
     "grade": false,
     "grade_id": "results-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|dest|retraso_medio|\n",
      "+----+-------------+\n",
      "| GRX|          2.6|\n",
      "+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agregadosDF.show()  # Ejecuta varias veces esta celda tras enviar el primer mensaje, hasta ver que el DataFrame no es vacío\n",
    "retraso_medio_GRX_primer_mensaje = 2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b8f02459cd58afae6cf1a32ef69f36e",
     "grade": false,
     "grade_id": "results-2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|dest|retraso_medio|\n",
      "+----+-------------+\n",
      "| GRX|          2.6|\n",
      "+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejecuta varias veces esta celda tras enviar el segundo mensaje, hasta ver que el DataFrame ha cambiado\n",
    "agregadosDF.show()\n",
    "retraso_medio_GRX_segundo_mensaje = 2.6\n",
    "retraso_medio_MAD_segundo_mensaje = 5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ed1d482d7859eaf314d85f7f056d79e",
     "grade": false,
     "grade_id": "results-3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|dest|retraso_medio|\n",
      "+----+-------------+\n",
      "| MAD|          5.4|\n",
      "| GRX|          2.6|\n",
      "+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejecuta varias veces esta celda tras enviar el tercer mensaje, hasta ver que el DataFrame ha cambiado\n",
    "agregadosDF.show()\n",
    "retraso_medio_GRX_tercer_mensaje = 2.05\n",
    "retraso_medio_MAD_tercer_mensaje = 5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e49b6258fd677f636bbc9f01e41a593",
     "grade": false,
     "grade_id": "results-4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|dest|retraso_medio|\n",
      "+----+-------------+\n",
      "| MAD|          5.4|\n",
      "| GRX|         2.05|\n",
      "+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejecuta varias veces esta celda tras enviar el cuarto mensaje, hasta ver que el DataFrame ha cambiado\n",
    "agregadosDF.show()\n",
    "retraso_medio_GRX_cuarto_mensaje = 2.05\n",
    "retraso_medio_MAD_cuarto_mensaje = 12.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "065e98ac2bd3061d7706cd282041e722",
     "grade": true,
     "grade_id": "results-tests",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
